---
title: "Generative AI Can't Replace Expertise"
subtitle: "It Exposes Its Absence"
author: "Ruth Yankson"
date: "today"
format:
  revealjs:
    theme: sky
    transition: slide
    background-transition: fade
    slide-number: true
    chalkboard: true
    footer: "Generative AI & Data Science Expertise"
    css: styles.css
---


### What is the Data Science Workflow? {background-color="#bbdefb"}

::: {.fragment}
::: {#fig-ds_workflow width="80%"}
![](img/ds_workflow.png)

Overview of the data science workflow showing five stages with AI assistance.

*Created with ChatGPT*
:::
:::

::: {.notes}
The data science workflow has five core stages: data collection and cleaning, exploratory analysis, modeling and evaluation, interpretation, and communication.

At each stage, critical decisions must be made: decisions that require understanding the why behind the techniques, not just the how. Gen AI can assist at every step, but here's the catch: it acts as a stress test of your understanding. If you don't know what questions to ask, you won't recognize when the answers are wrong.
:::

---

### How Can Gen AI Assist? {background-color="#90caf9"}

::: {.columns}
::: {.column width="33%"}
#### Data Collection

::: {.fragment}
✓ Suggests cleaning steps instantly
:::

::: {.fragment}
✗ **Without expertise:** Remove meaningful outliers, introduce bias
:::
:::

::: {.column width="33%"}
#### Visualization

::: {.fragment}
✓ Beautiful plots in seconds
:::

::: {.fragment}
✗ **Without context:** Misleading correlations, hidden confounders
:::
:::

::: {.column width="33%"}
#### Interpretation

::: {.fragment .highlight-red}
**Most Dangerous**
:::

::: {.fragment}
✗ Confident narratives with weak evidence

✗ Implies causality where only correlation exists
:::
:::
:::

::: {.notes}
Let's focus on three areas where Gen AI seems most helpful:

Data Collection: AI can suggest cleaning steps instantly—handling missing values, removing outliers. Sounds great, right? But without expertise, you might remove outliers that contain meaningful signals or impute missing data incorrectly, introducing bias.

Visualization: AI generates beautiful plots in seconds. However, a correlation plot without context can mislead faster than no plot at all. It might hide confounding variables or imply stability where none exists.

Interpretation: This is the most dangerous stage. Gen AI produces confident narratives even when evidence is weak. It can explain patterns that don't generalize and imply causality where only correlation exists.
:::

---

### A Tale of Two Outputs {background-color="#64b5f6"}

::: {.columns}
::: {.column width="50%"}
#### Beginner Approach

::: {.fragment}
- Used Gen AI blindly
- Got visually appealing chart
- Saw high accuracy $\rightarrow$ stopped questioning
:::

::: {.fragment .highlight-red}
**Hidden Problems:**

- Masked class imbalance
- Inappropriate metrics
- Unsupported conclusions
:::
:::

::: {.column width="50%"}
#### Expert Approach

::: {.fragment}
- Used same Gen AI tool
- Asked probing questions
- Verified assumptions
- Cross-checked domain knowledge
:::

::: {.fragment .highlight-current-blue}
**Same tool, dramatically different outcomes**
:::
:::
:::

<br>

::: {.r-fit-text .fragment}
**Why?** Gen AI mimics competence brilliantly, but cannot verify truth.
:::

::: {.notes}
Look at these two visualizations of the same dataset. 

The beginner used Gen AI and got a visually appealing chart—clean, colorful, professional-looking. They saw high accuracy scores and stopped questioning. But this chart masks class imbalance, uses inappropriate metrics, and draws conclusions the data can't support.

The expert used the same Gen AI tool differently. They asked probing questions, verified assumptions, cross-checked against domain knowledge, and identified when the AI's confident suggestions were conceptually wrong. Same tool, dramatically different outcomes.

Why? Because Gen AI mimics competence brilliantly, but cannot verify truth. The expert knew what to look for; the beginner didn't know what they didn't know.
:::

---

### Questions we Can't Skip {background-color="#42a5f5"}

::: {.incremental}
1. If Gen AI is so powerful, **why do two people get completely different results** from the same prompt?

2. When your AI-generated code runs successfully, **does that mean your analysis is correct?**

3. Can you identify when Gen AI **hallucinates** a statistical method or misapplies a technique?
:::

::: {.fragment}
::: {#fig-question width="70%"}
![](img/questionme.jpg)
:::
:::

::: {.notes}
Consider these questions:

1. If Gen AI is so powerful, why do two people get completely different results from the same prompt?
2. When your AI-generated code runs successfully, does that mean your analysis is correct?
3. Can you identify when Gen AI hallucinates a statistical method or misapplies a technique?
4. Who is responsible when an AI-suggested analysis leads to a biased or flawed decision?
5. If you can't explain why the AI recommended that approach, should you trust it?

[Pause here to let questions sink in]
:::

---

### Common Claims {background-color="#2196f3"}

::: {.columns}
::: {.column width="50%"}
#### Claim 1

*"Gen AI democratizes data science: anyone can do it now!"*

::: {.fragment .fade-in}
**Why it's weak:**

Democratizing access to tools $\neq$ democratizing understanding

Running code $\neq$ solving problems correctly
:::
:::

::: {.column width="50%"}
#### Claim 2

*"AI will just get better and handle edge cases."*

::: {.fragment .fade-in}
**Why it's weak:**

AI generates **statistically plausible** outputs, not **conceptually correct** ones

Cannot understand context, ethics, or domain nuances
:::
:::

::: {.notes}
Some argue: 'Gen AI democratizes data science: anyone can do it now!'

Weak because: Democratizing access to tools doesn't democratize understanding. Running code isn't the same as solving problems correctly.

Others say: 'AI will just get better and handle these edge cases.'

Weak because: AI generates statistically plausible outputs, not conceptually correct ones. It cannot understand real-world context, ethical constraints, or domain-specific nuances. That gap won't close simply by improving the model.
:::
:::

---

### This is Cargo-Cult Data Science

::: {.fragment}

Practices followed because they **look right**, not because they're **understood**

**Real-world consequences:** 

::: {.fragment}
- Flawed decisions 
- Biased systems 
- Ethical breaches
:::

:::

::: {.notes}
This is cargo-cult data science: practices followed because they look right, not because they're understood. The consequences in real-world settings include flawed decisions, biased systems, and ethical breaches.
:::

---

### Statistical Evidence {background-color="#1976d2"}

#### The Data Tells a Clear Story

```{r}
#| echo: false
#| fig-width: 10
#| fig-height: 5
library(ggplot2)

# Create data
df <- data.frame(
  category = c(
    "Don't Fully Trust\nAI-Generated Code",
    "Always Check Code\nBefore Using",
    "AI Code Appears Correct\nWhen It Contains Errors"
  ),
  percentage = c(96, 48, 61),
  group = c("trust", "check", "errors")
)

# Create bar plot
ggplot(df, aes(x = reorder(category, percentage), y = percentage, fill = group)) +
  geom_col(width = 0.7) +
  geom_text(aes(label = paste0(percentage, "%")), hjust = -0.2, size = 6) +
  coord_flip() +
  scale_y_continuous(limits = c(0, 105), breaks = seq(0, 100, 25)) +
  labs(x = "", y = "Percentage of Developers") +
  theme_minimal(base_size = 16) +
  theme(legend.position = "none")
```

::: {.fragment}
*Source: [Techradar](https://www.techradar.com/pro/devs-dont-trust-ai-code-but-many-say-they-still-dont-check-it-anyways?utm_source=chatgpt.com)*
:::

::: {.notes}
The data tells a clear story:

- 96% of software developers report they don't fully trust AI-generated code to be functionally correct
- Yet only 48% said they always check the code before using it
- 61% agreed that AI-generated code often appears correct even when it contains errors
:::

---

### Conclusion {background-color="#0d47a1"}

::: {.r-fit-text}
**A Question for You**
:::

<br>

::: {.fragment .fade-in-then-semi-out}
With these statistics in front of you, are you convinced enough to **up your game as an expert data scientist** by studying rigorously and critically evaluating AI outputs?
:::

<br>

::: {.fragment .fade-in-then-semi-out}
Or do you still want to stay complacent and **let Gen AI control how you solve problems?**
:::

::: {.notes}
Let me bring this home with a question:

So, with these statistics in front of you, are you convinced enough to up your game as an expert data scientist by studying rigorously and critically evaluating AI outputs? Or do you still want to stay complacent and let Gen AI control how you solve problems?
:::

---

### {background-color="#0d47a1"}

::: {.r-fit-text}
**Gen AI is a Mirror**
:::

<br>

It reflects the quality of thinking behind the prompt.

It accelerates **good** data science *and* **bad** data science alike.

::: {.notes}

Remember: Gen AI is a mirror. It reflects the quality of thinking behind the prompt. It accelerates good data science and bad data science alike.

:::

---

::: {.r-fit-text}
**Remember:**
:::

<br>

::: {.fragment}
::: {.incremental}
- If Gen AI makes your work **better**, it's because **you were already good**
- If it makes your work **worse**, it's not because the tool failed: it's because **it revealed what was missing**
:::
:::

---

::: {.fragment}
::: {.r-fit-text}
**Thank you.**
:::
:::

::: {.fragment}
![Using generative AI effectively requires active judgment and verification. Source: @giphy_student_using_ai.](https://media0.giphy.com/media/v1.Y2lkPTc5MGI3NjExa3ZmcmI4cTJtcjY0OGMxd25jem03emlldGxtMG91a2Nsa3RxMGtwYSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/imHJKSWL5rt8n0fZO8/giphy.gif)
:::


